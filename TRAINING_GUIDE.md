# SAM2-MoE Training Guide

SAM2 base_plus ì²´í¬í¬ì¸íŠ¸ë¥¼ initial parameterë¡œ ì‚¬ìš©í•˜ì—¬ MoE LoRA adapterë¥¼ fine-tuningí•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ë¹ ë¥¸ ì‹œì‘

### 1. ëª¨ë¸ ë¡œë“œ ë° íŒŒë¼ë¯¸í„° í™•ì¸

```python
from sam2.build_sam import build_sam2_video_predictor_moe
import torch

# SAM2-MoE ëª¨ë¸ ë¹Œë“œ (base_plus ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
predictor = build_sam2_video_predictor_moe(
    config_file="configs/sam2.1/sam2.1_hiera_b+_moe.yaml",
    ckpt_path="checkpoints/sam2.1_hiera_base_plus.pt",
    device="cuda",
    mode="train",
)

# íŒŒë¼ë¯¸í„° í†µê³„
total = sum(p.numel() for p in predictor.parameters())
trainable = sum(p.numel() for p in predictor.parameters() if p.requires_grad)

print(f"Total parameters: {total:,}")
print(f"Trainable parameters: {trainable:,}")
print(f"Trainable ratio: {100*trainable/total:.2f}%")
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
Total parameters: 81,510,978
Trainable parameters: 660,800
Trainable ratio: 0.81%
```

### 2. ê°„ë‹¨í•œ Training ì˜ˆì œ ì‹¤í–‰

```bash
# Dummy dataë¡œ í…ŒìŠ¤íŠ¸
uv run python examples/simple_train_moe.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
- âœ… Base weights ë¡œë“œ (`checkpoints/sam2.1_hiera_base_plus.pt`)
- âœ… Base model freeze, MoE adaptersë§Œ trainable
- âœ… Optimizer ì„¤ì •
- âœ… Training loop ì‹¤í–‰
- âœ… MoE adapter checkpoint ì €ì¥/ë¡œë“œ

## Training ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš©

```bash
# Dummy dataë¡œ í…ŒìŠ¤íŠ¸
uv run python train_moe.py \
    --config_file configs/sam2.1/sam2.1_hiera_b+_moe.yaml \
    --ckpt_path checkpoints/sam2.1_hiera_base_plus.pt \
    --output_dir outputs/moe_training \
    --num_epochs 10 \
    --batch_size 2 \
    --learning_rate 1e-4 \
    --use_dummy_data
```

### ì‹¤ì œ ë°ì´í„°ì…‹ ì‚¬ìš©

```bash
# ì‹¤ì œ í•™ìŠµ (ë°ì´í„°ì…‹ ì¤€ë¹„ í›„)
uv run python train_moe.py \
    --config_file configs/sam2.1/sam2.1_hiera_b+_moe.yaml \
    --ckpt_path checkpoints/sam2.1_hiera_base_plus.pt \
    --output_dir outputs/my_experiment \
    --num_epochs 20 \
    --batch_size 4 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 4
```

## ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

### MoE ì„¤ì • (config íŒŒì¼ì—ì„œ)

```yaml
# configs/sam2.1/sam2.1_hiera_b+_moe.yaml

self_attention:
  num_experts: 10      # Expert ê°œìˆ˜
  lora_rank: 4         # LoRA rank (bottleneck dimension)
  top_k: 2             # í™œì„±í™”í•  expert ìˆ˜
  lora_alpha: 1.0      # LoRA scaling factor
  lora_dropout: 0.1    # LoRA layer dropout
```

### Training ì„¤ì •

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `learning_rate` | 1e-4 | í•™ìŠµë¥  |
| `weight_decay` | 0.01 | Weight decay for AdamW |
| `batch_size` | 2 | Batch size |
| `gradient_accumulation_steps` | 4 | Gradient accumulation |
| `max_grad_norm` | 1.0 | Gradient clipping |
| `num_epochs` | 10 | ì „ì²´ epoch ìˆ˜ |

**Effective batch size** = `batch_size` Ã— `gradient_accumulation_steps`

## Checkpoint ê´€ë¦¬

### ìë™ ì €ì¥

Training ì¤‘ ìë™ìœ¼ë¡œ checkpointê°€ ì €ì¥ë©ë‹ˆë‹¤:

```
outputs/moe_training/
â”œâ”€â”€ checkpoint-1000.pt      # 1000 stepë§ˆë‹¤
â”œâ”€â”€ checkpoint-2000.pt
â”œâ”€â”€ checkpoint-epoch-1.pt   # Epochë§ˆë‹¤
â”œâ”€â”€ checkpoint-epoch-2.pt
â””â”€â”€ ...
```

### Checkpoint êµ¬ì¡°

```python
checkpoint = {
    'epoch': í˜„ì¬ epoch,
    'global_step': í˜„ì¬ step,
    'model_state_dict': MoE adapter weightsë§Œ,
    'optimizer_state_dict': optimizer ìƒíƒœ,
    'scheduler_state_dict': scheduler ìƒíƒœ,
}
```

### MoE Adapterë§Œ ì €ì¥/ë¡œë“œ

```python
# ì €ì¥
moe_state_dict = {
    name: param
    for name, param in model.state_dict().items()
    if any(k in name for k in ['lora', 'gate', 'experts'])
}
torch.save({'moe_state_dict': moe_state_dict}, 'moe_adapters.pt')

# ë¡œë“œ
checkpoint = torch.load('moe_adapters.pt')
model.load_state_dict(checkpoint['moe_state_dict'], strict=False)
```

## Fine-tuning ì „ëµ

### 1. Parameter-Efficient Fine-tuning

- **Base model**: Frozen (81M parameters)
- **LoRA adapters**: Trainable (660K parameters, 0.81%)
- **Gating networks**: Trainable

**ì¥ì :**
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  (gradientëŠ” 0.81%ë§Œ)
- ë¹ ë¥¸ í•™ìŠµ
- Catastrophic forgetting ì™„í™”

### 2. Learning Rate ì„¤ì •

```python
# Base modelì—ì„œ fine-tuningí•˜ë¯€ë¡œ ë‚®ì€ LR ì‚¬ìš©
optimizer = torch.optim.AdamW(
    [p for p in model.parameters() if p.requires_grad],
    lr=1e-4,  # ê¸°ë³¸ê°’
    weight_decay=0.01,
)

# Cosine annealing scheduler
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=num_epochs,
    eta_min=1e-6,
)
```

### 3. Gradient Accumulation

GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•  ë•Œ:

```bash
# Batch size ì¤„ì´ê³  accumulation ëŠ˜ë¦¬ê¸°
uv run python train_moe.py \
    --batch_size 1 \
    --gradient_accumulation_steps 8  # effective batch = 8
```

## Expert ë¶„ì„

### Expert Usage ëª¨ë‹ˆí„°ë§

```python
# Training loopì—ì„œ
if step % 500 == 0:
    # MoE attention ëª¨ë“ˆ ì°¾ê¸°
    for name, module in model.named_modules():
        if 'MoERoPEAttention' in str(type(module).__name__):
            # Dummy inputìœ¼ë¡œ expert usage í™•ì¸
            with torch.no_grad():
                stats = module.get_expert_statistics(q, k, v)

            # Expert weights ë¡œê¹…
            for i, weight in enumerate(stats['q_proj']):
                print(f"Expert {i}: {weight:.4f}")
```

### Expert Specialization í™•ì¸

Training í›„ ê° expertê°€ ì–´ë–¤ domain/objectì— íŠ¹í™”ë˜ì—ˆëŠ”ì§€ ë¶„ì„:

```python
import numpy as np

# ì—¬ëŸ¬ domainì—ì„œ expert usage ìˆ˜ì§‘
domains = ['medical', 'robotics', 'outdoor']
expert_usage = {domain: [] for domain in domains}

for domain in domains:
    data = load_domain_data(domain)
    for batch in data:
        usage = get_expert_usage(model, batch)
        expert_usage[domain].append(usage)

# Visualization
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 3, figsize=(15, 4))
for idx, domain in enumerate(domains):
    avg_usage = np.mean(expert_usage[domain], axis=0)
    axes[idx].bar(range(10), avg_usage)
    axes[idx].set_title(f'{domain} - Expert Usage')
    axes[idx].set_xlabel('Expert ID')
    axes[idx].set_ylabel('Usage')
plt.tight_layout()
plt.savefig('expert_specialization.png')
```

## ë°ì´í„°ì…‹ ì¤€ë¹„

### Custom Dataset êµ¬í˜„

```python
from torch.utils.data import Dataset

class VideoSegmentationDataset(Dataset):
    def __init__(self, video_dir, annotation_dir):
        self.video_dir = video_dir
        self.annotation_dir = annotation_dir
        # ... ì´ˆê¸°í™”

    def __len__(self):
        return len(self.videos)

    def __getitem__(self, idx):
        # ë¹„ë””ì˜¤ í”„ë ˆì„ ë¡œë“œ
        frames = self.load_frames(idx)  # (T, C, H, W)

        # Annotation (masks, points) ë¡œë“œ
        masks = self.load_masks(idx)    # (T, H, W)
        points = self.load_points(idx)   # (N, 2) or None

        return {
            'frames': frames,
            'masks': masks,
            'points': points,
        }
```

### DataLoader ì„¤ì •

```python
from torch.utils.data import DataLoader

dataset = VideoSegmentationDataset(
    video_dir='data/videos',
    annotation_dir='data/annotations',
)

dataloader = DataLoader(
    dataset,
    batch_size=2,
    shuffle=True,
    num_workers=4,
    pin_memory=True,
)
```

## Loss Function

### Segmentation Loss

```python
def compute_segmentation_loss(predictions, targets):
    # Dice Loss
    dice_loss = dice_loss_fn(predictions, targets)

    # Focal Loss
    focal_loss = focal_loss_fn(predictions, targets)

    # Total Loss
    total_loss = dice_loss + focal_loss

    return total_loss
```

### IoU Prediction Loss (optional)

```python
# SAM2ëŠ” IoUë„ ì˜ˆì¸¡í•˜ë¯€ë¡œ
iou_loss = F.mse_loss(pred_iou, target_iou)
total_loss = mask_loss + 0.1 * iou_loss
```

## ëª¨ë‹ˆí„°ë§ & ë””ë²„ê¹…

### Training ë¡œê·¸

```python
# train_moe.pyì—ì„œ ìë™ìœ¼ë¡œ ë¡œê¹…
if step % logging_steps == 0:
    print(f"Step {step}:")
    print(f"  Loss: {loss:.4f}")
    print(f"  LR: {scheduler.get_last_lr()[0]:.6f}")
    print(f"  Trainable params: {trainable_params:,}")
```

### TensorBoard (optional)

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('outputs/tensorboard')

# Training loopì—ì„œ
writer.add_scalar('Loss/train', loss, step)
writer.add_scalar('LR', lr, step)
writer.add_histogram('Expert/usage', expert_usage, step)
```

### WandB (optional)

```python
import wandb

wandb.init(project='sam2-moe', name='experiment_1')

# Training loopì—ì„œ
wandb.log({
    'loss': loss,
    'lr': lr,
    'expert_usage': expert_usage,
})
```

## ë¬¸ì œ í•´ê²°

### CUDA Out of Memory

```bash
# ì˜µì…˜ 1: Batch size ì¤„ì´ê¸°
--batch_size 1 --gradient_accumulation_steps 8

# ì˜µì…˜ 2: ì‘ì€ ëª¨ë¸ ì‚¬ìš©
--config_file configs/sam2.1/sam2.1_hiera_s_moe.yaml \
--ckpt_path checkpoints/sam2.1_hiera_small.pt
```

### Trainingì´ ë„ˆë¬´ ëŠë¦¼

```bash
# Mixed precision training í™œì„±í™”
--use_amp

# DataLoader workers ëŠ˜ë¦¬ê¸°
--num_workers 8
```

### Expertê°€ ê· ë“±í•˜ê²Œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

Load balancing loss ì¶”ê°€:

```python
# Encourage uniform expert usage
load_balance_loss = torch.var(expert_weights.mean(dim=(0, 1)))
total_loss = total_loss + 0.01 * load_balance_loss
```

## ì˜ˆì œ Scripts

### 1. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸

```bash
uv run python examples/simple_train_moe.py
```

### 2. Dummy dataë¡œ full training

```bash
uv run python train_moe.py --use_dummy_data --num_epochs 2
```

### 3. ì‹¤ì œ í•™ìŠµ

```bash
uv run python train_moe.py \
    --config_file configs/sam2.1/sam2.1_hiera_b+_moe.yaml \
    --ckpt_path checkpoints/sam2.1_hiera_base_plus.pt \
    --output_dir outputs/my_experiment \
    --num_epochs 20
```

## ì°¸ê³  ìë£Œ

- **ì „ì²´ ë¬¸ì„œ**: [SAM2_MOE_README.md](SAM2_MOE_README.md)
- **Quick Start**: [QUICKSTART.md](QUICKSTART.md)
- **Training Script**: [train_moe.py](train_moe.py)
- **Simple Example**: [examples/simple_train_moe.py](examples/simple_train_moe.py)

## ìš”ì•½

âœ… **Base weights ë¡œë“œ**: `checkpoints/sam2.1_hiera_base_plus.pt`
âœ… **MoE adaptersë§Œ í•™ìŠµ**: 0.81% of parameters
âœ… **Parameter-efficient**: 81M frozen, 660K trainable
âœ… **Catastrophic forgetting ì™„í™”**: Expert specialization
âœ… **ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸**: tiny, small, base_plus, large

Happy training! ğŸš€
