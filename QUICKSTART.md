# SAM2-MoE Quick Start Guide

ë¹ ë¥´ê²Œ SAM2-MoEë¥¼ ì‹œì‘í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
2. [ëª¨ë¸ ë¹Œë“œ & í…ŒìŠ¤íŠ¸](#ëª¨ë¸-ë¹Œë“œ--í…ŒìŠ¤íŠ¸)
3. [ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ](#ì²´í¬í¬ì¸íŠ¸-ë¡œë“œ)
4. [Fine-tuning ì‹œì‘](#fine-tuning-ì‹œì‘)
5. [Expert ë¶„ì„](#expert-ë¶„ì„)

---

## í™˜ê²½ ì„¤ì •

### 1. ì €ì¥ì†Œ í´ë¡  ë° ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ì´ë¯¸ í´ë¡ ë˜ì–´ ìˆë‹¤ë©´ ìŠ¤í‚µ
cd /home/jinu/github.com/doldam0/samoe

# uvë¡œ ì˜ì¡´ì„± ë™ê¸°í™”
uv sync
```

### 2. SAM2 ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ

```bash
# ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìˆë‹¤ë©´ ìŠ¤í‚µ)
bash checkpoints/download_ckpts.sh
```

í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:
- âœ“ `checkpoints/sam2.1_hiera_base_plus.pt` (309MB)
- âœ“ `checkpoints/sam2.1_hiera_large.pt` (857MB)
- âœ“ `checkpoints/sam2.1_hiera_small.pt` (176MB)
- âœ“ `checkpoints/sam2.1_hiera_tiny.pt` (149MB)

---

## ëª¨ë¸ ë¹Œë“œ & í…ŒìŠ¤íŠ¸

### ê°„ë‹¨í•œ ë°ëª¨ ì‹¤í–‰

```bash
# MoE ëª¨ë¸ ë¹Œë“œ ë° êµ¬ì¡° í™•ì¸
uv run python examples/sam2_moe_demo.py
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
Building SAM2 with Mixture of Prompt Experts (MoPE)
âœ“ Model built successfully!

Model Architecture Summary
Total parameters: 89,123,456
Trainable parameters (MoE adapters): 4,567,890
Frozen parameters (base model): 84,555,566
Trainable ratio: 5.12%

MoE Structure Analysis
Found 8 MoE-enhanced attention modules:
  - Num experts: 10
  - LoRA rank: 4
  - Top-k: 2
```

---

## ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

### Python ì½”ë“œì—ì„œ ì‚¬ìš©

```python
from sam2.build_sam import build_sam2_video_predictor_moe

# MoE ëª¨ë¸ ë¹Œë“œ (base_plus ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©)
predictor = build_sam2_video_predictor_moe(
    config_file="configs/sam2.1/sam2.1_hiera_b+_moe.yaml",
    ckpt_path="checkpoints/sam2.1_hiera_base_plus.pt",
    device="cuda",
    mode="eval",  # ë˜ëŠ” "train"
)

# íŒŒë¼ë¯¸í„° í™•ì¸
total = sum(p.numel() for p in predictor.parameters())
trainable = sum(p.numel() for p in predictor.parameters() if p.requires_grad)
print(f"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)")
```

**ì¤‘ìš” í¬ì¸íŠ¸:**
- âœ… Base model weightsëŠ” ìë™ìœ¼ë¡œ frozenë¨
- âœ… LoRA adaptersì™€ gating networksë§Œ í•™ìŠµ ê°€ëŠ¥
- âœ… ì•½ 5-10%ì˜ íŒŒë¼ë¯¸í„°ë§Œ trainable

---

## Fine-tuning ì‹œì‘

### ë°©ë²• 1: ê°„ë‹¨í•œ ì˜ˆì œ ì‹¤í–‰

```bash
# ê°„ë‹¨í•œ training ë°ëª¨ (dummy data ì‚¬ìš©)
uv run python examples/simple_train_moe.py
```

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
1. âœ“ MoE ëª¨ë¸ ë¡œë“œ
2. âœ“ Trainable parameter í™•ì¸
3. âœ“ Optimizer ì„¤ì •
4. âœ“ Training loop ì‹¤í–‰
5. âœ“ MoE adapter ì €ì¥/ë¡œë“œ

### ë°©ë²• 2: ì „ì²´ Training ìŠ¤í¬ë¦½íŠ¸

```bash
# ì‹¤ì œ í•™ìŠµìš© ìŠ¤í¬ë¦½íŠ¸ (ë°ì´í„°ì…‹ í•„ìš”)
uv run python train_moe.py \
    --config_file configs/sam2.1/sam2.1_hiera_b+_moe.yaml \
    --ckpt_path checkpoints/sam2.1_hiera_base_plus.pt \
    --output_dir outputs/moe_training \
    --num_epochs 10 \
    --batch_size 2 \
    --learning_rate 1e-4 \
    --use_dummy_data  # í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œ í•™ìŠµ ì‹œ ì œê±°)
```

### Training Configuration ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ

`configs/training/train_moe_config.yaml` íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ì„¤ì • ë³€ê²½:

```yaml
# MoE í•˜ì´í¼íŒŒë¼ë¯¸í„°
moe:
  num_experts: 10      # Expert ê°œìˆ˜
  lora_rank: 4         # LoRA rank
  top_k: 2             # í™œì„±í™”í•  expert ìˆ˜
  lora_alpha: 1.0      # LoRA scaling

# Training í•˜ì´í¼íŒŒë¼ë¯¸í„°
training:
  learning_rate: 1.0e-4
  batch_size: 2
  num_epochs: 10
```

---

## Expert ë¶„ì„

### Expert Usage ëª¨ë‹ˆí„°ë§

```python
from sam2.build_sam import build_sam2_video_predictor_moe
import torch

# ëª¨ë¸ ë¡œë“œ
predictor = build_sam2_video_predictor_moe(
    config_file="configs/sam2.1/sam2.1_hiera_b+_moe.yaml",
    ckpt_path="checkpoints/sam2.1_hiera_base_plus.pt",
    device="cuda",
)

# MoE attention ëª¨ë“ˆ ì°¾ê¸°
moe_modules = []
for name, module in predictor.named_modules():
    if "MoERoPEAttention" in str(type(module).__name__):
        moe_modules.append((name, module))

print(f"Found {len(moe_modules)} MoE attention modules")

# Dummy inputìœ¼ë¡œ expert usage í™•ì¸
dummy_q = torch.randn(1, 4096, 256).cuda()  # (B, N, D)
dummy_k = torch.randn(1, 4096, 256).cuda()
dummy_v = torch.randn(1, 4096, 256).cuda()

# ì²« ë²ˆì§¸ MoE ëª¨ë“ˆì˜ expert statistics
name, moe_module = moe_modules[0]
with torch.no_grad():
    stats = moe_module.get_expert_statistics(dummy_q, dummy_k, dummy_v)

print(f"\nExpert usage for {name}:")
print("Q projection:")
for i, weight in enumerate(stats['q_proj']):
    print(f"  Expert {i}: {weight.item():.4f}")
```

### Visualization (optional)

```python
import matplotlib.pyplot as plt
import numpy as np

# Expert weights ì‹œê°í™”
weights = stats['q_proj'].cpu().numpy()

plt.figure(figsize=(10, 5))
plt.bar(range(len(weights)), weights)
plt.xlabel('Expert ID')
plt.ylabel('Average Weight')
plt.title('Expert Usage Distribution (Q Projection)')
plt.savefig('expert_usage.png')
print("Saved to expert_usage.png")
```

---

## ë‹¤ìŒ ë‹¨ê³„

### 1. ë°ì´í„°ì…‹ ì¤€ë¹„

ì‹¤ì œ í•™ìŠµì„ ìœ„í•´ video object segmentation ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ì„¸ìš”:

```python
# ì˜ˆì‹œ: Custom dataset êµ¬í˜„
class VideoSegmentationDataset:
    def __getitem__(self, idx):
        return {
            'frames': torch.Tensor,      # (T, C, H, W)
            'masks': torch.Tensor,       # (T, H, W)
            'points': torch.Tensor,      # (N, 2) ë˜ëŠ” None
        }
```

### 2. Training Loop ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆ

`train_moe.py`ì˜ `train_step()` ë©”ì„œë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ì‹¤ì œ SAM2 inference API ì‚¬ìš©:

```python
def train_step(self, batch):
    frames = batch['frames']
    masks = batch['masks']

    # SAM2 inference ì‚¬ìš©
    inference_state = self.model.init_state(video_path=...)
    self.model.add_new_points(inference_state, points=...)
    predictions = self.model.propagate_in_video(inference_state)

    # Loss ê³„ì‚°
    loss = compute_segmentation_loss(predictions, masks)
    return loss
```

### 3. Expert Specialization ë¶„ì„

í•™ìŠµ í›„ ì–´ë–¤ expertê°€ ì–´ë–¤ domain/objectì— íŠ¹í™”ë˜ì—ˆëŠ”ì§€ ë¶„ì„:

```python
# ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ expert usage ë¹„êµ
domains = ['medical', 'robotics', 'autonomous_driving']
for domain in domains:
    data = load_domain_data(domain)
    usage = analyze_expert_usage(model, data)
    print(f"{domain}: {usage}")
```

---

## ì°¸ê³  ìë£Œ

- **ì „ì²´ ë¬¸ì„œ**: [SAM2_MOE_README.md](SAM2_MOE_README.md)
- **Training ìŠ¤í¬ë¦½íŠ¸**: [train_moe.py](train_moe.py)
- **ê°„ë‹¨í•œ ì˜ˆì œ**: [examples/simple_train_moe.py](examples/simple_train_moe.py)
- **Configuration**: [configs/training/train_moe_config.yaml](configs/training/train_moe_config.yaml)

---

## ë¬¸ì œ í•´ê²°

### CUDA Out of Memory

```bash
# Batch size ì¤„ì´ê¸°
uv run python train_moe.py --batch_size 1 --gradient_accumulation_steps 8

# ë˜ëŠ” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
uv run python train_moe.py \
    --config_file configs/sam2.1/sam2.1_hiera_t_moe.yaml \
    --ckpt_path checkpoints/sam2.1_hiera_tiny.pt
```

### Import Error

```bash
# ì˜ì¡´ì„± ì¬ì„¤ì¹˜
uv sync --reinstall
```

### Checkpoint Loading Error

MoE adapter keysê°€ ì—†ëŠ” ê²ƒì€ ì •ìƒì…ë‹ˆë‹¤ (ì²˜ìŒ í•™ìŠµ ì‹œ):
```
MoE adapter keys not loaded (expected): 1234 keys
```

ì´ëŠ” base weightsë§Œ ë¡œë“œë˜ê³  LoRA adaptersëŠ” random initializationë˜ì—ˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

---

## ìš”ì•½

```bash
# 1. ë°ëª¨ ì‹¤í–‰
uv run python examples/sam2_moe_demo.py

# 2. ê°„ë‹¨í•œ training í…ŒìŠ¤íŠ¸
uv run python examples/simple_train_moe.py

# 3. ì „ì²´ training (dummy data)
uv run python train_moe.py --use_dummy_data --num_epochs 2

# 4. ì‹¤ì œ training (ë°ì´í„° ì¤€ë¹„ í›„)
uv run python train_moe.py \
    --config_file configs/sam2.1/sam2.1_hiera_b+_moe.yaml \
    --ckpt_path checkpoints/sam2.1_hiera_base_plus.pt \
    --num_epochs 10
```

Happy training! ğŸš€
